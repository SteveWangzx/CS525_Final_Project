{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SteveWangzx/CS525_Final_Project/blob/main/news_seq2seq_summarization_no_cross.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Data"
      ],
      "metadata": {
        "id": "gZJZ5eE63MKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "otaua_aa27M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141023c1-31e7-416b-fb46-ad159ac8439e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/NYT_Dataset.csv')"
      ],
      "metadata": {
        "id": "FjeCpM2z4Chj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocess"
      ],
      "metadata": {
        "id": "cXw-Hb7y277u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "id": "iIWu72TS4Lyu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36f8cc12-390d-4b81-fce7-827b8ecc0bbd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# clean text\n",
        "\n",
        "import re\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Process text function.\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "    \"\"\"\n",
        "    text=str(text)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    text= re.sub('\\[[^]]*\\]', '', text)\n",
        "    # remove stock market tickers like $GE\n",
        "    text = re.sub(r'\\$\\w*', '', text)\n",
        "    #removal of html tags\n",
        "    review =re.sub(r'<.*?>',' ',text) \n",
        "    # remove old style retweet text \"RT\"\n",
        "    text = re.sub(r'^RT[\\s]+', '', text)\n",
        "    # remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    # remove hashtags\n",
        "    # only removing the hash # sign from the word\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = re.sub(\"[\"u\"\\U0001F600-\\U0001F64F\"  # removal of emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\",' ',text)\n",
        "    text = re.sub('[^a-zA-Z]',' ',text) \n",
        "    text = text.lower()\n",
        "    text_tokens =word_tokenize(text)\n",
        "\n",
        "    text_clean = []\n",
        "    for word in  text_tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation):  # remove punctuation\n",
        "            lem_word =lemmatizer.lemmatize(word)  # lemmitiging word\n",
        "            text_clean.append(lem_word)\n",
        "    text_mod=[i for i in text_clean if len(i)>2]\n",
        "    text_clean=' '.join(text_mod)\n",
        "    text_clean = '<start> ' + text_clean + ' <end>'\n",
        "    return  text_clean"
      ],
      "metadata": {
        "id": "8Jy3ZDhc4MVF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e981da65-cff2-4649-9859-a655e899860c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ddf=df[['title','abstract','keywords']].reset_index()\n",
        "\n",
        "ddf['title']=ddf['title'].apply(lambda x:clean_text(x))\n",
        "ddf['abstract']=ddf['abstract'].apply(lambda x: clean_text(x))"
      ],
      "metadata": {
        "id": "PbZnCJ3j4QKN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddf['keywords'][0]"
      ],
      "metadata": {
        "id": "miGlVgCX4Xsh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "c10927b4-d1d5-45aa-de26-548d8b7517d9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"['Assassinations and Attempted Assassinations', 'Pakistan', 'Bhutto, Benazir', 'Federal Bureau of Investigation', 'United Nations']\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def join_word(text):\n",
        "  text=str(text)\n",
        "  list=text.strip('][').split(',')\n",
        "  clean_text=[]\n",
        "  for i in list:\n",
        "    clean_text.append(i.replace('\\'',''))\n",
        "  text_clean=' '.join(clean_text)\n",
        "  return text_clean"
      ],
      "metadata": {
        "id": "bGceU3_U4ZRt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddf['keywords']=ddf['keywords'].apply(lambda x:join_word(x))\n",
        "ddf['keywords']=ddf['keywords'].apply(lambda x:clean_text(x))"
      ],
      "metadata": {
        "id": "7PDikvlT4Z-2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ddf['keywords'][0]"
      ],
      "metadata": {
        "id": "8U2hgmhZ8Cfp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "696f8cba-f592-4873-df85-adc0d4b4341b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<start> assassination attempted assassination pakistan bhutto benazir federal bureau investigation united nation <end>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow-addons"
      ],
      "metadata": {
        "id": "btOWHgSoIrMp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60e69d2e-3836-448f-ac52-9d2a5220054c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.19.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 26.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow-addons) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.19.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time"
      ],
      "metadata": {
        "id": "-0TTHmcK4bZt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(lang):\n",
        "  lang=lang.apply(lambda x:str(x))\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='<OOV>')\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang) \n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "  return tensor, lang_tokenizer"
      ],
      "metadata": {
        "id": "r0lQFTW74gdN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor, inp_lang_tokenizer = tokenize(ddf['abstract'])\n",
        "target_tensor, tar_lang_tokenizer = tokenize(ddf['title'])\n",
        "key_tensor, key_lang_tokenizer = tokenize(ddf['keywords'])"
      ],
      "metadata": {
        "id": "56DbML0y4ul0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_tensor_train, input_tensor_test, target_tensor_train, target_tensor_test,key_tensor_train,key_tensor_test = \\\n",
        "train_test_split(input_tensor, target_tensor, key_tensor, test_size=0.2)"
      ],
      "metadata": {
        "id": "8AXXQjHy40-q"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(input_tensor_train.shape, key_tensor_train.shape, target_tensor_train.shape)"
      ],
      "metadata": {
        "id": "sHvtydGBeZD8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4bcaac7-d696-499a-a017-a377afa68770"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(85204, 132) (85204, 175) (85204, 17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BUFFER_SIZE = 32000\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "pqoNWMgH42Nu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Dataset for Train"
      ],
      "metadata": {
        "id": "dD1EMsW7cIvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, key_tensor_train, target_tensor_train))\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((input_tensor_test, key_tensor_test, target_tensor_test))\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "j53q_FOzeqx0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Input Example"
      ],
      "metadata": {
        "id": "hMdDKN1_c1tG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_input_batch, example_key_batch, example_target_batch = next(iter(train_dataset))\n",
        "example_input_batch.shape, example_key_batch.shape, example_target_batch.shape"
      ],
      "metadata": {
        "id": "4ELwCVWj4456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2a45ad-95f6-421e-9e74-1b93cf752223"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 132]), TensorShape([64, 175]), TensorShape([64, 17]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp_vocab_size = len(inp_lang_tokenizer.word_index)+1\n",
        "tar_vocab_size = len(tar_lang_tokenizer.word_index)+1\n",
        "max_length_input = example_input_batch.shape[1]\n",
        "max_length_key = example_key_batch.shape[1]\n",
        "max_length_output = example_target_batch.shape[1]\n",
        "\n",
        "embedding_dim = 256\n",
        "units = 512\n",
        "steps_per_epoch = num_examples=30000"
      ],
      "metadata": {
        "id": "Me53VysO46EL"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length_input, max_length_output, max_length_key, inp_vocab_size, tar_vocab_size"
      ],
      "metadata": {
        "id": "qZJyq15747Za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5fff79a-c400-4551-ae80-7d410f8b476d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(132, 17, 175, 43658, 25870)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encode"
      ],
      "metadata": {
        "id": "ZjDkfuDB49HM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.enc_units = enc_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    ##-------- LSTM layer in Encoder ------- ##\n",
        "    self.lstm_layer = tf.keras.layers.LSTM(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "\n",
        "\n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "    output, h, c = self.lstm_layer(x, initial_state = hidden)\n",
        "    return output, h, c\n",
        "\n",
        "  def initialize_hidden_state(self):\n",
        "    return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]"
      ],
      "metadata": {
        "id": "TXX92UTF48wm"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder Test Input"
      ],
      "metadata": {
        "id": "C4yRZX_sfbB0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Test Encoder Stack\n",
        "\n",
        "encoder = Encoder(inp_vocab_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_h, sample_c = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder h vecotr shape: (batch size, units) {}'.format(sample_h.shape))\n",
        "print ('Encoder c vector shape: (batch size, units) {}'.format(sample_c.shape))\n"
      ],
      "metadata": {
        "id": "lOdxpN8i4_8y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b177eba5-9da6-48f1-b886-2e8a6bea3d1c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 132, 512)\n",
            "Encoder h vecotr shape: (batch size, units) (64, 512)\n",
            "Encoder c vector shape: (batch size, units) (64, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross Attention"
      ],
      "metadata": {
        "id": "smo9tdiXAz09"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class CrossAttention(tf.keras.layers.Layer):\n",
        "#   def __init__(self, units, **kwargs):\n",
        "#     super().__init__()\n",
        "#     self.mha = tf.keras.layers.MultiHeadAttention(key_dim=units, num_heads=2, **kwargs)\n",
        "\n",
        "#   def call(self, key_word, context):\n",
        "#     attn_output, attn_scores = self.mha(\n",
        "#         query=key_word,\n",
        "#         value=context,\n",
        "#         return_attention_scores=True)\n",
        "\n",
        "#     # Cache the attention scores for plotting later.\n",
        "#     attn_scores = tf.reduce_mean(attn_scores, axis=1)\n",
        "#     self.last_attention_weights = attn_scores\n",
        "\n",
        "#     return attn_output"
      ],
      "metadata": {
        "id": "iY1V36CBA2Hl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder With Two Attention Layers: Self & Cross"
      ],
      "metadata": {
        "id": "ustkkpG1cIkn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cross -> Self"
      ],
      "metadata": {
        "id": "p7GstOWcdwGe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "2xMXTqMDcF3W"
      },
      "outputs": [],
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, attention_type='luong'):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz\n",
        "    self.dec_units = dec_units\n",
        "    self.attention_type = attention_type\n",
        "    \n",
        "    # Embedding Layer\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    #Final Dense layer on which softmax will be applied\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # Define the fundamental cell for decoder recurrent structure\n",
        "    self.decoder_rnn_cell = tf.keras.layers.LSTMCell(self.dec_units)\n",
        "\n",
        "    # Sampler\n",
        "    self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
        "\n",
        "    # Create attention mechanism with memory = None\n",
        "    self.attention_mechanism = self.build_attention_mechanism(self.dec_units, \n",
        "                                None, self.batch_sz*[max_length_input], self.attention_type)\n",
        "\n",
        "    # Wrap attention mechanism with the fundamental rnn cell of decoder\n",
        "    self.rnn_cell = self.build_rnn_cell(batch_sz)\n",
        "\n",
        "    # Define the decoder with respect to fundamental rnn cell\n",
        "    self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell, sampler=self.sampler, output_layer=self.fc)\n",
        "\n",
        "    \n",
        "  def build_rnn_cell(self, batch_sz):\n",
        "    rnn_cell = tfa.seq2seq.AttentionWrapper(self.decoder_rnn_cell, \n",
        "                          self.attention_mechanism, attention_layer_size=self.dec_units)\n",
        "    return rnn_cell\n",
        "\n",
        "  def build_attention_mechanism(self, dec_units, memory, memory_sequence_length, attention_type='luong'):\n",
        "    # ------------- #\n",
        "    # typ: Which sort of attention (Bahdanau, Luong)\n",
        "    # dec_units: final dimension of attention outputs \n",
        "    # memory: encoder hidden states of shape (batch_size, max_length_input, enc_units)\n",
        "    # memory_sequence_length: 1d array of shape (batch_size) with every element set to max_length_input (for masking purpose)\n",
        "\n",
        "    if(attention_type=='bahdanau'):\n",
        "      return tfa.seq2seq.BahdanauAttention\\\n",
        "      (units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "    else:\n",
        "      return tfa.seq2seq.LuongAttention\\\n",
        "      (units=dec_units, memory=memory, memory_sequence_length=memory_sequence_length)\n",
        "\n",
        "  def build_initial_state(self, batch_sz, encoder_state, Dtype):\n",
        "    decoder_initial_state = self.rnn_cell.get_initial_state(batch_size=batch_sz, dtype=Dtype)\n",
        "    decoder_initial_state = decoder_initial_state.clone(cell_state=encoder_state)\n",
        "    return decoder_initial_state\n",
        "\n",
        "  def call(self, inputs, initial_state):\n",
        "    x = self.embedding(inputs)\n",
        "    outputs, _, _ = self.decoder(x, initial_state=initial_state, sequence_length=self.batch_sz*[max_length_output-1])\n",
        "    return outputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self -> Cross"
      ],
      "metadata": {
        "id": "6_hzi9Rkdhjl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test decoder stack\n",
        "\n",
        "decoder = Decoder(tar_vocab_size, embedding_dim, units, BATCH_SIZE, 'luong')\n",
        "struct_output = tf.random.uniform((BATCH_SIZE, max_length_output))"
      ],
      "metadata": {
        "id": "glYo07cHaR9Z"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder.attention_mechanism.setup_memory(sample_output)\n",
        "initial_state = decoder.build_initial_state(BATCH_SIZE, [sample_h, sample_c], tf.float32)\n",
        "\n",
        "\n",
        "sample_decoder_outputs = decoder(struct_output, initial_state)\n",
        "\n",
        "print(\"Decoder Outputs Shape: \", sample_decoder_outputs.rnn_output.shape)"
      ],
      "metadata": {
        "id": "oeF6UsBqG2e_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb6bdf9f-860b-422c-9bc5-11fd1018e4d0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Outputs Shape:  (64, 16, 25870)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Optimizer & Loss"
      ],
      "metadata": {
        "id": "uhe2WIMiG5JN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  # real shape = (BATCH_SIZE, max_length_output)\n",
        "  # pred shape = (BATCH_SIZE, max_length_output, tar_vocab_size )\n",
        "  cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "  loss = cross_entropy(y_true=real, y_pred=pred)\n",
        "  mask = tf.logical_not(tf.math.equal(real,0))   #output 0 for y=0 else output 1\n",
        "  mask = tf.cast(mask, dtype=loss.dtype)  \n",
        "  loss = mask* loss\n",
        "  loss = tf.reduce_mean(loss)\n",
        "  return loss  "
      ],
      "metadata": {
        "id": "F2Ujvjd7m9HY"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                encoder=encoder, decoder=decoder)"
      ],
      "metadata": {
        "id": "Zre56XL6drEu"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Operation in One Iteration"
      ],
      "metadata": {
        "id": "XhloxolDnJ0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inp, key, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_h, enc_c = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_input = targ[ : , :-1 ] # Ignore <end> token\n",
        "    real = targ[ : , 1: ]         # ignore <start> token\n",
        "\n",
        "    # Set the AttentionMechanism object with encoder_outputs\n",
        "    decoder.attention_mechanism.setup_memory(enc_output)\n",
        "\n",
        "    # Create AttentionWrapperState as initial_state for decoder\n",
        "    decoder_initial_state = decoder.build_initial_state(BATCH_SIZE, [enc_h, enc_c], tf.float32)\n",
        "    pred = decoder(dec_input, decoder_initial_state)\n",
        "    logits = pred.rnn_output\n",
        "    loss = loss_function(real, logits)\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "  gradients = tape.gradient(loss, variables)\n",
        "  optimizer.apply_gradients(zip(gradients, variables))\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "F_aC1f_FnGsR"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "8HKT96L9nQJL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "  # print(enc_hidden[0].shape, enc_hidden[1].shape)\n",
        "\n",
        "  for (batch, (inp, key, targ)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, key, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-j_OUN-snOkn",
        "outputId": "f19ac21f-a7c5-431a-fe05-4598ed48241b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 4.5843\n",
            "Epoch 1 Batch 100 Loss 3.4914\n",
            "Epoch 1 Batch 200 Loss 3.2356\n",
            "Epoch 1 Batch 300 Loss 3.2771\n",
            "Epoch 1 Batch 400 Loss 3.2087\n",
            "Epoch 1 Batch 500 Loss 3.3105\n",
            "Epoch 1 Batch 600 Loss 2.8800\n",
            "Epoch 1 Batch 700 Loss 2.9407\n",
            "Epoch 1 Batch 800 Loss 2.9540\n",
            "Epoch 1 Batch 900 Loss 2.9153\n",
            "Epoch 1 Batch 1000 Loss 2.7447\n",
            "Epoch 1 Batch 1100 Loss 2.7343\n",
            "Epoch 1 Batch 1200 Loss 2.9507\n",
            "Epoch 1 Batch 1300 Loss 2.8056\n",
            "Epoch 1 Loss 0.1356\n",
            "Time taken for 1 epoch 147.10779356956482 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.7973\n",
            "Epoch 2 Batch 100 Loss 2.6955\n",
            "Epoch 2 Batch 200 Loss 2.5147\n",
            "Epoch 2 Batch 300 Loss 2.5878\n",
            "Epoch 2 Batch 400 Loss 2.6021\n",
            "Epoch 2 Batch 500 Loss 2.6498\n",
            "Epoch 2 Batch 600 Loss 2.8771\n",
            "Epoch 2 Batch 700 Loss 2.6109\n",
            "Epoch 2 Batch 800 Loss 2.4637\n",
            "Epoch 2 Batch 900 Loss 2.4684\n",
            "Epoch 2 Batch 1000 Loss 2.4297\n",
            "Epoch 2 Batch 1100 Loss 2.5734\n",
            "Epoch 2 Batch 1200 Loss 2.3043\n",
            "Epoch 2 Batch 1300 Loss 2.4688\n",
            "Epoch 2 Loss 0.1167\n",
            "Time taken for 1 epoch 137.35430335998535 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 2.4513\n",
            "Epoch 3 Batch 100 Loss 2.4730\n",
            "Epoch 3 Batch 200 Loss 2.4112\n",
            "Epoch 3 Batch 300 Loss 2.2739\n",
            "Epoch 3 Batch 400 Loss 2.4839\n",
            "Epoch 3 Batch 500 Loss 2.3009\n",
            "Epoch 3 Batch 600 Loss 2.4108\n",
            "Epoch 3 Batch 700 Loss 2.4765\n",
            "Epoch 3 Batch 800 Loss 2.3602\n",
            "Epoch 3 Batch 900 Loss 2.2339\n",
            "Epoch 3 Batch 1000 Loss 2.4787\n",
            "Epoch 3 Batch 1100 Loss 2.3934\n",
            "Epoch 3 Batch 1200 Loss 2.2191\n",
            "Epoch 3 Batch 1300 Loss 2.4064\n",
            "Epoch 3 Loss 0.1045\n",
            "Time taken for 1 epoch 136.25390315055847 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 2.1193\n",
            "Epoch 4 Batch 100 Loss 2.0317\n",
            "Epoch 4 Batch 200 Loss 1.8690\n",
            "Epoch 4 Batch 300 Loss 2.2497\n",
            "Epoch 4 Batch 400 Loss 2.1995\n",
            "Epoch 4 Batch 500 Loss 2.1706\n",
            "Epoch 4 Batch 600 Loss 2.0025\n",
            "Epoch 4 Batch 700 Loss 2.0076\n",
            "Epoch 4 Batch 800 Loss 2.2875\n",
            "Epoch 4 Batch 900 Loss 2.2512\n",
            "Epoch 4 Batch 1000 Loss 2.0539\n",
            "Epoch 4 Batch 1100 Loss 2.1873\n",
            "Epoch 4 Batch 1200 Loss 2.0900\n",
            "Epoch 4 Batch 1300 Loss 2.1101\n",
            "Epoch 4 Loss 0.0943\n",
            "Time taken for 1 epoch 136.87183547019958 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 2.1265\n",
            "Epoch 5 Batch 100 Loss 1.9074\n",
            "Epoch 5 Batch 200 Loss 2.2308\n",
            "Epoch 5 Batch 300 Loss 1.9263\n",
            "Epoch 5 Batch 400 Loss 1.8509\n",
            "Epoch 5 Batch 500 Loss 1.9633\n",
            "Epoch 5 Batch 600 Loss 1.8349\n",
            "Epoch 5 Batch 700 Loss 1.7918\n",
            "Epoch 5 Batch 800 Loss 2.0905\n",
            "Epoch 5 Batch 900 Loss 1.9720\n",
            "Epoch 5 Batch 1000 Loss 1.9925\n",
            "Epoch 5 Batch 1100 Loss 1.9476\n",
            "Epoch 5 Batch 1200 Loss 1.8793\n",
            "Epoch 5 Batch 1300 Loss 1.9234\n",
            "Epoch 5 Loss 0.0844\n",
            "Time taken for 1 epoch 136.128666639328 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.6490\n",
            "Epoch 6 Batch 100 Loss 1.6069\n",
            "Epoch 6 Batch 200 Loss 1.6837\n",
            "Epoch 6 Batch 300 Loss 1.7970\n",
            "Epoch 6 Batch 400 Loss 1.7128\n",
            "Epoch 6 Batch 500 Loss 1.7665\n",
            "Epoch 6 Batch 600 Loss 1.7297\n",
            "Epoch 6 Batch 700 Loss 1.5271\n",
            "Epoch 6 Batch 800 Loss 1.7736\n",
            "Epoch 6 Batch 900 Loss 1.6870\n",
            "Epoch 6 Batch 1000 Loss 1.7181\n",
            "Epoch 6 Batch 1100 Loss 1.7211\n",
            "Epoch 6 Batch 1200 Loss 1.6130\n",
            "Epoch 6 Batch 1300 Loss 1.7714\n",
            "Epoch 6 Loss 0.0746\n",
            "Time taken for 1 epoch 136.64289546012878 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.4614\n",
            "Epoch 7 Batch 100 Loss 1.5495\n",
            "Epoch 7 Batch 200 Loss 1.3422\n",
            "Epoch 7 Batch 300 Loss 1.4608\n",
            "Epoch 7 Batch 400 Loss 1.4226\n",
            "Epoch 7 Batch 500 Loss 1.4221\n",
            "Epoch 7 Batch 600 Loss 1.5160\n",
            "Epoch 7 Batch 700 Loss 1.2523\n",
            "Epoch 7 Batch 800 Loss 1.2798\n",
            "Epoch 7 Batch 900 Loss 1.5257\n",
            "Epoch 7 Batch 1000 Loss 1.2812\n",
            "Epoch 7 Batch 1100 Loss 1.4111\n",
            "Epoch 7 Batch 1200 Loss 1.2750\n",
            "Epoch 7 Batch 1300 Loss 1.4935\n",
            "Epoch 7 Loss 0.0654\n",
            "Time taken for 1 epoch 136.02347421646118 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.4234\n",
            "Epoch 8 Batch 100 Loss 1.1995\n",
            "Epoch 8 Batch 200 Loss 1.2176\n",
            "Epoch 8 Batch 300 Loss 1.1894\n",
            "Epoch 8 Batch 400 Loss 1.3434\n",
            "Epoch 8 Batch 500 Loss 1.2945\n",
            "Epoch 8 Batch 600 Loss 1.1063\n",
            "Epoch 8 Batch 700 Loss 1.2625\n",
            "Epoch 8 Batch 800 Loss 1.3185\n",
            "Epoch 8 Batch 900 Loss 1.3076\n",
            "Epoch 8 Batch 1000 Loss 1.4162\n",
            "Epoch 8 Batch 1100 Loss 1.2359\n",
            "Epoch 8 Batch 1200 Loss 1.4179\n",
            "Epoch 8 Batch 1300 Loss 1.2625\n",
            "Epoch 8 Loss 0.0572\n",
            "Time taken for 1 epoch 136.59607911109924 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.0724\n",
            "Epoch 9 Batch 100 Loss 1.0021\n",
            "Epoch 9 Batch 200 Loss 1.0890\n",
            "Epoch 9 Batch 300 Loss 1.0171\n",
            "Epoch 9 Batch 400 Loss 1.0633\n",
            "Epoch 9 Batch 500 Loss 1.3149\n",
            "Epoch 9 Batch 600 Loss 1.1683\n",
            "Epoch 9 Batch 700 Loss 1.0540\n",
            "Epoch 9 Batch 800 Loss 1.0900\n",
            "Epoch 9 Batch 900 Loss 1.0938\n",
            "Epoch 9 Batch 1000 Loss 1.0120\n",
            "Epoch 9 Batch 1100 Loss 1.1263\n",
            "Epoch 9 Batch 1200 Loss 1.1971\n",
            "Epoch 9 Batch 1300 Loss 1.1592\n",
            "Epoch 9 Loss 0.0498\n",
            "Time taken for 1 epoch 136.1782591342926 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.0322\n",
            "Epoch 10 Batch 100 Loss 0.9727\n",
            "Epoch 10 Batch 200 Loss 0.9440\n",
            "Epoch 10 Batch 300 Loss 1.0104\n",
            "Epoch 10 Batch 400 Loss 0.9861\n",
            "Epoch 10 Batch 500 Loss 0.8869\n",
            "Epoch 10 Batch 600 Loss 0.9949\n",
            "Epoch 10 Batch 700 Loss 1.0123\n",
            "Epoch 10 Batch 800 Loss 0.9364\n",
            "Epoch 10 Batch 900 Loss 0.9015\n",
            "Epoch 10 Batch 1000 Loss 1.0138\n",
            "Epoch 10 Batch 1100 Loss 0.9525\n",
            "Epoch 10 Batch 1200 Loss 1.1645\n",
            "Epoch 10 Batch 1300 Loss 0.9130\n",
            "Epoch 10 Loss 0.0434\n",
            "Time taken for 1 epoch 136.67248463630676 sec\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid = ddf[:1000]\n",
        "\n",
        "x = valid['abstract'].to_numpy()\n",
        "y = valid['title'].to_numpy()\n",
        "gene_valid = valid.to_numpy()\n",
        "gene_valid[0]"
      ],
      "metadata": {
        "id": "FkyaIc-26e4D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5027981-cd3d-4673-af70-6e3bb969f080"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0,\n",
              "       '<start> reversal pakistan welcome outside help inquiry bhutto <end>',\n",
              "       '<start> pakistan ambassador said government would endorse separate inquiry modeled one carried assassination rafik hariri lebanon <end>',\n",
              "       '<start> assassination attempted assassination pakistan bhutto benazir federal bureau investigation united nation <end>'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "def evaluate_sentence(sentence, key_word):\n",
        "  inputs = [inp_lang_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                          maxlen=max_length_input,\n",
        "                                                          padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "  inference_batch_size = inputs.shape[0]\n",
        "\n",
        "  # key_inputs = [key_lang_tokenizer.word_index[i] for i in key_word.split(' ')]\n",
        "  # key_inputs = tf.keras.preprocessing.sequence.pad_sequences([key_inputs],\n",
        "  #                                                         maxlen=max_length_key,\n",
        "  #                                                         padding='post')\n",
        "  # key_inputs = tf.convert_to_tensor(key_inputs)\n",
        "  # key_batch_size = key_inputs.shape[0]\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  enc_start_state = [tf.zeros((inference_batch_size, units)), tf.zeros((inference_batch_size,units))]\n",
        "  enc_out, enc_h, enc_c = encoder(inputs, enc_start_state)\n",
        "\n",
        "  # key_start_state = [tf.zeros((key_batch_size, units)), tf.zeros((key_batch_size,units))]\n",
        "  # enc_key_out, _, _ = key_encoder(key_inputs, key_start_state)\n",
        "\n",
        "  dec_h = enc_h\n",
        "  dec_c = enc_c\n",
        "\n",
        "  start_tokens = tf.fill([inference_batch_size], tar_lang_tokenizer.word_index['<start>'])\n",
        "  end_token = tar_lang_tokenizer.word_index['<end>']\n",
        "\n",
        "  greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n",
        "\n",
        "  # Instantiate BasicDecoder object\n",
        "  decoder_instance = tfa.seq2seq.BasicDecoder(cell=decoder.rnn_cell, sampler=greedy_sampler, output_layer=decoder.fc)\n",
        "\n",
        "  # # Cross Attention\n",
        "  # cross_output = decoder.run_cross_attn(enc_out, enc_key_out)\n",
        "\n",
        "  # Setup Memory in decoder stack\n",
        "  decoder.attention_mechanism.setup_memory(enc_out)\n",
        "\n",
        "  # set decoder_initial_state\n",
        "  decoder_initial_state = decoder.build_initial_state(inference_batch_size, [enc_h, enc_c], tf.float32)\n",
        "\n",
        "\n",
        "  ### Since the BasicDecoder wraps around Decoder's rnn cell only, you have to ensure that the inputs to BasicDecoder \n",
        "  ### decoding step is output of embedding layer. tfa.seq2seq.GreedyEmbeddingSampler() takes care of this. \n",
        "  ### You only need to get the weights of embedding layer, which can be done by decoder.embedding.variables[0] and pass this callabble to BasicDecoder's call() function\n",
        "\n",
        "  decoder_embedding_matrix = decoder.embedding.variables[0]\n",
        "  \n",
        "  outputs, _, _ = decoder_instance(decoder_embedding_matrix, start_tokens = start_tokens, end_token= end_token, initial_state=decoder_initial_state)\n",
        "  return outputs.sample_id.numpy()\n"
      ],
      "metadata": {
        "id": "tCsPqt6T6Cc3"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge"
      ],
      "metadata": {
        "id": "yEy6w9Vr6JRU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46ee521d-8ee0-40b5-dc08-fe427e10431c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from rouge) (1.15.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge import Rouge\n",
        "import numpy as np\n",
        "rouge = Rouge()\n",
        "\n",
        "def rouge_score(title, sentence, key_word):\n",
        "  result = evaluate_sentence(sentence, key_word)\n",
        "  result = tar_lang_tokenizer.sequences_to_texts(result)\n",
        "  title = title.replace('<start> ', '')\n",
        "  title = title.replace(' <end>', '')\n",
        "  result[0] = result[0].replace('<start> ', '')\n",
        "  result[0] = result[0].replace(' <end>', '')\n",
        "  scores = rouge.get_scores(result[0], title)\n",
        "  max_score = max(scores[0]['rouge-1']['p'], scores[0]['rouge-1']['f'], scores[0]['rouge-1']['r'], \n",
        "                  scores[0]['rouge-2']['p'],scores[0]['rouge-2']['f'], scores[0]['rouge-2']['r'])\n",
        "  return max_score\n",
        "  \n",
        "\n",
        "scores = []\n",
        "for rec in gene_valid:\n",
        "  keyword = rec[3]\n",
        "  keyword = keyword.replace('<start> ', '')\n",
        "  keyword = keyword.replace(' <end>', '')\n",
        "  if keyword == '':\n",
        "    continue\n",
        "  score = rouge_score(rec[1], rec[2], rec[3])\n",
        "  scores.append(score)\n",
        "\n",
        "print(np.mean(scores))"
      ],
      "metadata": {
        "id": "oLhgpwXd6Le_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ee5d3a0-dada-4ea2-f1af-4b8606f079b1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.43421839940164547\n"
          ]
        }
      ]
    }
  ]
}